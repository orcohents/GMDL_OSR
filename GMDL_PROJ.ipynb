{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comment it out and add the files to the current directory\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "# import sys\n",
    "# sys.path.insert(0,'./drive/MyDrive/Colab_Notebooks/GMDL/GMDL_HW5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/nirmu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.openSetClassifier import openSetClassifier\n",
    "from train_cacOpenset import train_osr\n",
    "from eval_cacOpenset import eval_osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# remove slow mirror from list of MNIST mirrors\n",
    "# torchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\n",
    "#                                       if not mirror.startswith(\"http://yann.lecun.com\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "config = dict(\n",
    "    epochs=2,\n",
    "    alpha_magnitude = 10,\n",
    "    num_classes=10,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=5e-6,\n",
    "    lbda_anchor_loss=0.1,\n",
    "    train_ratio = 0.8,\n",
    "    resume=False,\n",
    "    init_weights=True,\n",
    "    dataset=\"MNIST\",\n",
    "    architecture=\"CNN\",\n",
    "    use_wandb_flag=True,\n",
    "    project_name=\"GMDL_PROJ\",\n",
    "    run_name='run1',\n",
    "    trial=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_name='MNIST'):\n",
    "  # Define the mandatory transformations\n",
    "    mnist_mean = 0.1307\n",
    "    mnist_std = 0.3081\n",
    "    mnist_flip = 0\n",
    "    mnist_rotate = 0\n",
    "    mnist_scale_min = 0.7\n",
    "    if data_name == 'MNIST':\n",
    "        scale_min = mnist_scale_min\n",
    "        flip = mnist_flip\n",
    "        rotate = mnist_rotate\n",
    "        means = mnist_mean\n",
    "        stds = mnist_std\n",
    "        transform_dict = {\n",
    "                'train': transforms.Compose([\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.RandomResizedCrop((28,28), scale = (scale_min, 1.0)),\n",
    "                    transforms.RandomHorizontalFlip(flip),\n",
    "                    transforms.RandomRotation(rotate),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(means, stds)\n",
    "                ]),\n",
    "                'val': transforms.Compose([\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(means, stds)\n",
    "                ]),\n",
    "                'test': transforms.Compose([\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(means, stds)\n",
    "                ])\n",
    "            }\n",
    "    else:\n",
    "        transform_ood = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)) # OOD mean and std\n",
    "    ])\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset =None, None, None\n",
    "    if data_name == 'MNIST':\n",
    "        train_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                                  train=True,\n",
    "                                                  transform=transform_dict['train'],\n",
    "                                                  download=True)\n",
    "        val_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                                  train=True,\n",
    "                                                  transform=transform_dict['val'],\n",
    "                                                  download=True)\n",
    "        test_dataset = torchvision.datasets.MNIST(root=\"./data\",\n",
    "                                                  train=False,\n",
    "                                                  transform=transform_dict['test'],\n",
    "                                                  download=True)\n",
    "    elif data_name == 'FMNIST':\n",
    "        test_dataset = torchvision.datasets.FashionMNIST(root=\"./data\",\n",
    "                                                train=False,\n",
    "                                                transform=transform_ood,\n",
    "                                                download=True)\n",
    "    else:\n",
    "        # CIFAR10\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root=\"./data\",\n",
    "                                                  train=False,\n",
    "                                                  transform=transform_ood,\n",
    "                                                  download=True)\n",
    "\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "def make_loaders(config, data_name='MNIST'):\n",
    "    # CIFAR10 and FMNIST return only test_dataset\n",
    "    # Make the data\n",
    "    train_dataset, val_dataset, test_dataset = get_data(data_name)\n",
    "    train_loader = make_loader(train_dataset, batch_size=config.batch_size)\n",
    "    val_loader = make_loader(val_dataset, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test_dataset, batch_size=config.batch_size)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_OSR_model(init_weights_flag):\n",
    "    print('==> Building Network..')\n",
    "    model = openSetClassifier(init_weights_flag)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline_aux(hyperparameters, use_wandb_flag=False, wandb=None):\n",
    "    if use_wandb_flag:\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "    else:\n",
    "      config = AttrDict(hyperparameters)\n",
    "\n",
    "    # make the model, data, and optimization problem\n",
    "    train_loader, val_loader, test_loader = make_loaders(config)\n",
    "    model = make_OSR_model(config.init_weights)\n",
    "    optimizer = optim.SGD(model.parameters(), lr = config.learning_rate, \n",
    "\t\t\t\t\t\t\t\tmomentum = 0.9, weight_decay = config.weight_decay)\n",
    "\n",
    "    best_model_wts = train_osr(train_loader, val_loader, model, device, config, optimizer, wandb)\n",
    "\n",
    "\n",
    "    # and test its final performance\n",
    "    best_model = make_OSR_model(False)\n",
    "    best_model.load_state_dict(best_model_wts) # load the best model for validation\n",
    "    eval_osr(train_loader, test_loader, best_model, device, config, wandb)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "  use_wandb_flag = hyperparameters['use_wandb_flag']\n",
    "  if use_wandb_flag:\n",
    "    with wandb.init(project=hyperparameters['project_name'], name=hyperparameters['run_name'], config=hyperparameters):\n",
    "      return model_pipeline_aux(hyperparameters, use_wandb_flag, wandb)\n",
    "  else:\n",
    "      return model_pipeline_aux(hyperparameters, use_wandb_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nirmu/Semester2/GMDL/GMDL_PROJ/wandb/run-20230701_192504-8c9z7965</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bgu-vil/GMDL_PROJ/runs/8c9z7965' target=\"_blank\">run1</a></strong> to <a href='https://wandb.ai/bgu-vil/GMDL_PROJ' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bgu-vil/GMDL_PROJ' target=\"_blank\">https://wandb.ai/bgu-vil/GMDL_PROJ</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bgu-vil/GMDL_PROJ/runs/8c9z7965' target=\"_blank\">https://wandb.ai/bgu-vil/GMDL_PROJ/runs/8c9z7965</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building Anchores..\n",
      "\n",
      "Epoch: 1\n",
      "\n",
      "Epoch inside: 1\n",
      " [================================================================>]  Step: 43ms | Tot: 29s757ms | Loss: 1.308 | Acc: 80.670% (48402/6000 469/469 ...................................]  Step: 61ms | Tot: 4s151ms | Loss: 2.395 | Acc: 48.290% (4265/883 69/469 .....]  Step: 59ms | Tot: 5s49ms | Loss: 2.275 | Acc: 52.137% (5539/1062 83/469 91/469 .]  Step: 71ms | Tot: 6s578ms | Loss: 2.111 | Acc: 57.083% (7745/1356 106/469 ............................]  Step: 63ms | Tot: 6s777ms | Loss: 2.095 | Acc: 57.619% (8039/1395 109/469 ==============>.................................................]  Step: 65ms | Tot: 6s842ms | Loss: 2.089 | Acc: 57.777% (8135/1408 110/469 120/469 ..................................]  Step: 63ms | Tot: 8s159ms | Loss: 1.989 | Acc: 60.968% (10067/1651 129/469 .]  Step: 71ms | Tot: 8s359ms | Loss: 1.972 | Acc: 61.500% (10391/1689 132/469 .........................................]  Step: 66ms | Tot: 8s813ms | Loss: 1.941 | Acc: 62.461% (11113/1779 139/469 ======================>..........................................]  Step: 62ms | Tot: 10s233ms | Loss: 1.852 | Acc: 65.251% (13447/2060 161/469 ===========>.....................................]  Step: 60ms | Tot: 13s295ms | Loss: 1.722 | Acc: 69.089% (17952/2598 203/469 .]  Step: 85ms | Tot: 14s919ms | Loss: 1.657 | Acc: 71.049% (20826/2931 229/469 ============================>...............................]  Step: 66ms | Tot: 15s774ms | Loss: 1.629 | Acc: 71.891% (22177/3084 241/469 267/469 =====================================>..........................]  Step: 70ms | Tot: 18s458ms | Loss: 1.550 | Acc: 74.096% (26651/3596 281/469 =============================>....................]  Step: 82ms | Tot: 21s58ms | Loss: 1.481 | Acc: 76.056% (31542/4147 324/469 ====================================>...................]  Step: 58ms | Tot: 21s177ms | Loss: 1.478 | Acc: 76.124% (31765/4172 326/469 \n",
      " [================================================================>]  Step: 31ms | Tot: 19s522ms | Acc: 96.777% (58066/6000 468/469 .]  Step: 44ms | Tot: 485ms | Acc: 96.695% (1609/166 13/469 31/469 ....................]  Step: 39ms | Tot: 1s464ms | Acc: 96.516% (4571/473 37/469 ....................]  Step: 39ms | Tot: 1s504ms | Acc: 96.505% (4694/486 38/469 >...........................................................]  Step: 38ms | Tot: 1s626ms | Acc: 96.570% (5068/524 41/469 =>...........................................................]  Step: 39ms | Tot: 1s665ms | Acc: 96.596% (5193/537 42/469 ===>...........................................................]  Step: 40ms | Tot: 1s705ms | Acc: 96.584% (5316/550 43/4 46/469 ........................]  Step: 46ms | Tot: 2s332ms | Acc: 96.623% (7297/755 59/469 =======>.......................................................]  Step: 50ms | Tot: 2s664ms | Acc: 96.662% (8166/844 66/469 =====>......................................................]  Step: 41ms | Tot: 3s176ms | Acc: 96.608% (9398/972 76/469 ......]  Step: 41ms | Tot: 3s335ms | Acc: 96.611% (9893/1024 80/469 ......]  Step: 40ms | Tot: 3s376ms | Acc: 96.586% (10014/1036 81/469 ..................................]  Step: 45ms | Tot: 3s666ms | Acc: 96.618% (10883/1126 88/469 ============>....................................................]  Step: 41ms | Tot: 3s917ms | Acc: 96.617% (11625/1203 94/469 104/469 ...................]  Step: 44ms | Tot: 5s557ms | Acc: 96.632% (16327/1689 132/46 137/469 ....]  Step: 42ms | Tot: 5s796ms | Acc: 96.632% (17069/1766 138/469 142/469 ===================>.............................................]  Step: 41ms | Tot: 6s74ms | Acc: 96.649% (17938/1856 145/469 ===============>............................................]  Step: 42ms | Tot: 6s116ms | Acc: 96.640% (18060/1868 146/469 ............]  Step: 39ms | Tot: 6s317ms | Acc: 96.632% (18677/1932 151/469 152/469 153/469 ==================>...........................................]  Step: 42ms | Tot: 6s440ms | Acc: 96.631% (19048/1971 154/469 .......]  Step: 40ms | Tot: 6s560ms | Acc: 96.621% (19417/2009 157/469 ......]  Step: 40ms | Tot: 6s600ms | Acc: 96.613% (19539/2022 158/469 ====================>...........................................]  Step: 40ms | Tot: 6s641ms | Acc: 96.615% (19663/2035 159/469 ====================>..........................................]  Step: 40ms | Tot: 6s682ms | Acc: 96.626% (19789/2048 160/469 ======================>..........................................]  Step: 41ms | Tot: 6s762ms | Acc: 96.639% (20039/2073 162/469 ======================>..........................................]  Step: 40ms | Tot: 6s803ms | Acc: 96.655% (20166/2086 163/469 =====================>..........................................]  Step: 40ms | Tot: 6s883ms | Acc: 96.667% (20416/2112 165/469 .]  Step: 38ms | Tot: 7s335ms | Acc: 96.675% (21779/2252 176/469 177/469 ..............]  Step: 42ms | Tot: 7s417ms | Acc: 96.686% (22029/2278 178/469 ======================>........................................]  Step: 40ms | Tot: 7s458ms | Acc: 96.692% (22154/2291 179/46 188/469 .........]  Step: 40ms | Tot: 7s878ms | Acc: 96.668% (23386/2419 189/469 ......]  Step: 39ms | Tot: 7s918ms | Acc: 96.661% (23508/2432 190/46 193/469 ============>......................................]  Step: 41ms | Tot: 8s80ms | Acc: 96.658% (24002/2483 194/469 ======================>.....................................]  Step: 40ms | Tot: 8s162ms | Acc: 96.676% (24254/2508 196/469 ======================>.....................................]  Step: 40ms | Tot: 8s323ms | Acc: 96.688% (24752/2560 200/469 ==================>.....................................]  Step: 41ms | Tot: 8s365ms | Acc: 96.681% (24874/2572 201/469   Step: 39ms | Tot: 8s790ms | Acc: 96.697% (26116/2700 211/469   Step: 40ms | Tot: 8s830ms | Acc: 96.702% (26241/2713 212/469 ========================>...................................]  Step: 41ms | Tot: 8s871ms | Acc: 96.703% (26365/2726 213/46 219/46 220/469 ==============================>..................................]  Step: 39ms | Tot: 9s237ms | Acc: 96.731% (27487/2841 222/469 ==============================>..................................]  Step: 40ms | Tot: 9s278ms | Acc: 96.721% (27608/2854 223/469 =========================>..................................]  Step: 41ms | Tot: 9s320ms | Acc: 96.732% (27735/2867 224/469 .........................]  Step: 39ms | Tot: 9s775ms | Acc: 96.722% (29094/3008 235/46 236/469 237/469 =================================>...............................]  Step: 39ms | Tot: 10s182ms | Acc: 96.741% (30338/3136 245/469 ===============================>...............................]  Step: 39ms | Tot: 10s222ms | Acc: 96.732% (30459/3148 246/46 247/469 ]  Step: 39ms | Tot: 11s73ms | Acc: 96.702% (33049/3417 267/469 .]  Step: 41ms | Tot: 11s114ms | Acc: 96.706% (33174/3430 268/469 =======================================>.........................]  Step: 43ms | Tot: 11s958ms | Acc: 96.697% (35770/3699 289/469 ========================================>........................]  Step: 39ms | Tot: 12s159ms | Acc: 96.689% (36386/3763 294/469 ===================================>........................]  Step: 39ms | Tot: 12s240ms | Acc: 96.690% (36634/3788 296/469 .]  Step: 40ms | Tot: 12s698ms | Acc: 96.653% (37857/3916 306/469 ================================>....................]  Step: 40ms | Tot: 13s234ms | Acc: 96.667% (39471/4083 319/469 ===========================================>...................]  Step: 40ms | Tot: 13s681ms | Acc: 96.669% (40833/4224 330/469 346/469 ......]  Step: 40ms | Tot: 14s603ms | Acc: 96.671% (43680/4518 353/469 ===============================================>................]  Step: 41ms | Tot: 14s644ms | Acc: 96.670% (43803/4531 354/469 ================================================>..............]  Step: 50ms | Tot: 15s155ms | Acc: 96.688% (45049/4659 364/469 ======>.............]  Step: 40ms | Tot: 15s585ms | Acc: 96.706% (46295/4787 374/469 ========================================>.............]  Step: 39ms | Tot: 15s624ms | Acc: 96.704% (46418/4800 375/469 =================================================>.............]  Step: 41ms | Tot: 15s665ms | Acc: 96.705% (46542/4812 376/469 ====================================================>............]  Step: 39ms | Tot: 15s744ms | Acc: 96.714% (46794/4838 378/469 .....]  Step: 39ms | Tot: 15s784ms | Acc: 96.718% (46920/4851 379/469 ==================================================>............]  Step: 40ms | Tot: 15s824ms | Acc: 96.717% (47043/4864 380/469 =========================================>............]  Step: 41ms | Tot: 15s865ms | Acc: 96.715% (47166/4876 381/469 ===============================================>..........]  Step: 40ms | Tot: 16s538ms | Acc: 96.765% (49172/5081 397/469 =>........]  Step: 39ms | Tot: 16s953ms | Acc: 96.779% (50418/5209 407/469 ======================================================>........]  Step: 42ms | Tot: 16s995ms | Acc: 96.781% (50543/5222 408/469 =========================================================>.......]  Step: 38ms | Tot: 17s361ms | Acc: 96.761% (51647/5337 417/469 ===>.......]  Step: 39ms | Tot: 17s401ms | Acc: 96.767% (51774/5350 418/469 ========================================================>......]  Step: 39ms | Tot: 17s483ms | Acc: 96.762% (52019/5376 420/469 ============================================================>....]  Step: 45ms | Tot: 18s340ms | Acc: 96.754% (54492/5632 440/469 ============================================================>....]  Step: 40ms | Tot: 18s380ms | Acc: 96.749% (54613/5644 441/469 ==================>...]  Step: 40ms | Tot: 18s539ms | Acc: 96.763% (55116/5696 445/4 446/46 447/469 ==>...]  Step: 40ms | Tot: 18s658ms | Acc: 96.763% (55488/5734 448/469 =============================================================>..]  Step: 41ms | Tot: 18s700ms | Acc: 96.765% (55613/5747 449/469 ==============================================================>..]  Step: 40ms | Tot: 18s740ms | Acc: 96.771% (55740/5760 450/469 ============================================================>..]  Step: 42ms | Tot: 18s783ms | Acc: 96.768% (55862/5772 451/46 469/469 \n",
      "Saving..\n",
      "Saving..\n",
      "Saving..\n",
      "in val wandb logger, epoch 1\n",
      "{'val/accuracy': 96.77666666666667, 'val/anchorLoss': 4.93356466293335, 'val/CACLoss': 0.5907489061355591}\n",
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch inside: 2\n",
      " [================================================================>]  Step: 44ms | Tot: 28s356ms | Loss: 0.774 | Acc: 93.443% (56066/6000 469/469 ..............................]  Step: 59ms | Tot: 4s70ms | Loss: 0.864 | Acc: 91.916% (8118/883 69/46 77/46 93/46 159/469 ..................]  Step: 57ms | Tot: 11s418ms | Loss: 0.829 | Acc: 92.551% (22153/2393 187/46 213/46 280/469 =======================================>.......................]  Step: 59ms | Tot: 18s70ms | Loss: 0.805 | Acc: 92.966% (35461/3814 298/469 \n",
      " [================================================================>]  Step: 35ms | Tot: 19s559ms | Acc: 97.898% (58739/6000 469/469 ..........................................................]  Step: 47ms | Tot: 375ms | Acc: 97.969% (1254/128 10/469 ==>..............................................................]  Step: 46ms | Tot: 656ms | Acc: 97.978% (2132/217 17/469 >..............................................................]  Step: 40ms | Tot: 857ms | Acc: 97.869% (2756/281 22/469 =>.............................................................]  Step: 41ms | Tot: 899ms | Acc: 97.894% (2882/294 23/469 =>.............................................................]  Step: 40ms | Tot: 1s59ms | Acc: 98.032% (3388/345 27/469 29/469 ====>............................................................]  Step: 38ms | Tot: 1s340ms | Acc: 98.024% (4266/4 34/469 ....]  Step: 44ms | Tot: 1s385ms | Acc: 98.013% (4391/448 35/469 ..................]  Step: 39ms | Tot: 1s424ms | Acc: 97.960% (4514/460 36/469 ...........]  Step: 39ms | Tot: 1s464ms | Acc: 97.952% (4639/473 37/469 ===>...........................................................]  Step: 38ms | Tot: 1s503ms | Acc: 97.944% (4764/486 38/469 .....]  Step: 39ms | Tot: 1s543ms | Acc: 97.937% (4889/499 39/469 .................................................]  Step: 44ms | Tot: 1s625ms | Acc: 97.961% (5141/524 41/469 ====>..........................................................]  Step: 40ms | Tot: 1s827ms | Acc: 97.962% (5768/588 46/469 ..................]  Step: 40ms | Tot: 1s867ms | Acc: 97.972% (5894/601 47/469 =====>..........................................................]  Step: 40ms | Tot: 1s907ms | Acc: 97.982% (6020/614 48/469 54/469 65/469 ..............]  Step: 40ms | Tot: 2s620ms | Acc: 97.940% (8274/844 66/469 ========>.......................................................]  Step: 40ms | Tot: 2s661ms | Acc: 97.948% (8400/857 67/469 ...........]  Step: 40ms | Tot: 3s263ms | Acc: 97.856% (10271/1049 82/469 =====>...................................................]  Step: 39ms | Tot: 3s899ms | Acc: 97.850% (12149/1241 97/469 ===========>...................................................]  Step: 40ms | Tot: 3s939ms | Acc: 97.864% (12276/1254 98/469 ===========>...................................................]  Step: 40ms | Tot: 4s21ms | Acc: 97.859% (12526/1280 100/469 ==============>..................................................]  Step: 40ms | Tot: 4s310ms | Acc: 97.839% (13400/1369 107/469 ==============>.................................................]  Step: 41ms | Tot: 4s595ms | Acc: 97.814% (14273/1459 114/469 ==============>................................................]  Step: 39ms | Tot: 4s796ms | Acc: 97.834% (14902/1523 119/469 =================>...............................................]  Step: 40ms | Tot: 5s81ms | Acc: 97.842% (15780/1612 126/469 ================>...............................................]  Step: 40ms | Tot: 5s122ms | Acc: 97.841% (15905/1625 127/469 136/469 =========>..............................................]  Step: 40ms | Tot: 5s547ms | Acc: 97.850% (17159/1753 137/469 =================>.............................................]  Step: 45ms | Tot: 5s836ms | Acc: 97.862% (18038/1843 144/469 160/469 164/469 ....]  Step: 42ms | Tot: 6s857ms | Acc: 97.857% (20918/2137 167/469 ==================>......................................]  Step: 48ms | Tot: 7s997ms | Acc: 97.834% (24169/2470 193/469 ....]  Step: 52ms | Tot: 8s98ms | Acc: 97.829% (24418/2496 195/469 ========================>.....................................]  Step: 51ms | Tot: 8s149ms | Acc: 97.832% (24544/2508 196/469 215/469 ==================================>..............................]  Step: 40ms | Tot: 10s486ms | Acc: 97.843% (31435/3212 251/469 ====================>..............................]  Step: 40ms | Tot: 10s527ms | Acc: 97.839% (31559/3225 252/469 ================================>..............................]  Step: 41ms | Tot: 10s568ms | Acc: 97.829% (31681/3238 253/469 ===========================>.............................]  Step: 40ms | Tot: 10s608ms | Acc: 97.835% (31808/3251 254/469 ==================================>............................]  Step: 40ms | Tot: 10s976ms | Acc: 97.846% (32939/3366 263/469 ====================================>............................]  Step: 39ms | Tot: 11s16ms | Acc: 97.849% (33065/3379 264/469 284/469 285/469 286/469 .......]  Step: 40ms | Tot: 12s4ms | Acc: 97.852% (35947/3673 287/469 ==================================>.........................]  Step: 40ms | Tot: 12s44ms | Acc: 97.854% (36073/3686 288/469 ======================================>.........................]  Step: 41ms | Tot: 12s86ms | Acc: 97.854% (36198/3699 289/469 ]  Step: 39ms | Tot: 12s125ms | Acc: 97.858% (36325/3712 290/469 ................]  Step: 40ms | Tot: 12s590ms | Acc: 97.843% (37697/3852 301/46 302/46 311/469 ........]  Step: 41ms | Tot: 13s40ms | Acc: 97.852% (39078/3993 312/469 =========================================>.....................]  Step: 40ms | Tot: 13s80ms | Acc: 97.848% (39202/4006 313/469 =========================================>.....................]  Step: 39ms | Tot: 13s120ms | Acc: 97.848% (39327/4019 314/469 ....]  Step: 39ms | Tot: 13s610ms | Acc: 97.843% (40828/4172 326/469 ===>...................]  Step: 41ms | Tot: 13s810ms | Acc: 97.847% (41456/4236 331/469 =============================================>.................]  Step: 40ms | Tot: 14s311ms | Acc: 97.845% (42958/4390 343/46 356/469 364/469 365/469 =>..............]  Step: 42ms | Tot: 15s358ms | Acc: 97.869% (45975/4697 367/469 ===============================================>............]  Step: 40ms | Tot: 15s767ms | Acc: 97.874% (47230/4825 377/469 ====================================================>..........]  Step: 38ms | Tot: 16s613ms | Acc: 97.886% (49742/5081 397/469 ...]  Step: 40ms | Tot: 16s653ms | Acc: 97.884% (49866/5094 398/469 ======================================================>........]  Step: 40ms | Tot: 17s62ms | Acc: 97.875% (51114/5222 408/469 ========================================================>......]  Step: 39ms | Tot: 17s678ms | Acc: 97.859% (52985/5414 423/469 ================>......]  Step: 41ms | Tot: 17s719ms | Acc: 97.855% (53108/5427 424/469 ========================================================>......]  Step: 41ms | Tot: 17s760ms | Acc: 97.857% (53234/5440 425/469 ========================================================>......]  Step: 40ms | Tot: 17s801ms | Acc: 97.856% (53359/5452 426/469 ===========================================================>....]  Step: 39ms | Tot: 18s340ms | Acc: 97.863% (54991/5619 439/46 441/469 ===>...]  Step: 40ms | Tot: 18s461ms | Acc: 97.870% (55371/5657 442/469 ===========================================================>...]  Step: 40ms | Tot: 18s542ms | Acc: 97.867% (55620/5683 444/469 446/469 ============================================================>...]  Step: 41ms | Tot: 18s663ms | Acc: 97.880% (56003/5721 447/469 ================>..]  Step: 40ms | Tot: 18s991ms | Acc: 97.888% (57010/5824 455/469 =========================================================>.]  Step: 40ms | Tot: 19s31ms | Acc: 97.889% (57136/5836 456/469 =============================================================>.]  Step: 40ms | Tot: 19s71ms | Acc: 97.890% (57262/5849 457/469 ===============================================================>.]  Step: 40ms | Tot: 19s112ms | Acc: 97.890% (57387/5862 458/469 =====================================================>.]  Step: 40ms | Tot: 19s152ms | Acc: 97.893% (57514/5875 459/469 \n",
      "Saving..\n",
      "Saving..\n",
      "Saving..\n",
      "in val wandb logger, epoch 2\n",
      "{'val/accuracy': 97.89833333333333, 'val/anchorLoss': 3.872070074081421, 'val/CACLoss': 0.4731947183609009}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>train/CAC_Loss</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▂▁▂▂▁▂▁▁</td></tr><tr><td>train/anchor_Loss</td><td>█▇▅▅▅▄▄▄▄▄▃▄▃▄▃▃▃▂▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>train/tuplet_Loss</td><td>█▆▄▄▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train/CAC_Loss</td><td>0.60055</td></tr><tr><td>train/anchor_Loss</td><td>5.06698</td></tr><tr><td>train/tuplet_Loss</td><td>0.09385</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">run1</strong> at: <a href='https://wandb.ai/bgu-vil/GMDL_PROJ/runs/8c9z7965' target=\"_blank\">https://wandb.ai/bgu-vil/GMDL_PROJ/runs/8c9z7965</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230701_192504-8c9z7965/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gmdl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
